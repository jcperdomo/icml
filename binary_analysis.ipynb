{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from linear_models import LinearBinaryClassifier, LinearOneVsAllClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders = ['oracle', 'randomAscent', 'greedyAscent', 'gradientDescent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_histories = []\n",
    "max_acc_histories = []\n",
    "\n",
    "for folder in folders:\n",
    "    lh = np.load('binary_experiments/binary-'+folder+\"-1-17/loss_history.npy\")\n",
    "    loss_histories.append(lh)\n",
    "    ma = np.load('binary_experiments/binary-'+folder+\"-1-17/max_acc_history.npy\")\n",
    "    max_acc_histories.append(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"binary_data_2\"\n",
    "models = []\n",
    "exp_type = \"binary\"\n",
    "num_classifiers = 5\n",
    "alpha = .5\n",
    "\n",
    "X_exp = np.load(data_path + \"/\" + \"X_exp.npy\")\n",
    "Y_exp = np.load(data_path + \"/\" + \"Y_exp.npy\")\n",
    "\n",
    "for i in xrange(num_classifiers):\n",
    "    weights = np.load(data_path + \"/\" + \"weights_{}.npy\".format(i))\n",
    "    bias = np.load(data_path + \"/\" + \"bias_{}.npy\".format(i))\n",
    "    if exp_type == \"binary\":\n",
    "        model = LinearBinaryClassifier(weights, bias)\n",
    "    else:\n",
    "        model = LinearOneVsAllClassifier(10, weights, bias)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_weights = sum([1.0 / num_classifiers * model.weights for model in models ])\n",
    "ensemble_bias = sum([1.0 / num_classifiers * model.bias for model in models ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_model = LinearBinaryClassifier(ensemble_weights, ensemble_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model.evaluate(X_exp, Y_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(noise, alpha):\n",
    "    for i in range(noise.shape[0]):\n",
    "        noise[i] = noise[i] / np.linalg.norm(noise[i]) * alpha\n",
    "    return noise\n",
    "\n",
    "noise_ensemble =  -1 * ensemble_model.gradient(X_exp, Y_exp)\n",
    "noise_ensemble = normalize(noise_ensemble, alpha)\n",
    "\n",
    "individual_noise = [-1 * model.gradient(X_exp, Y_exp) for model in models]\n",
    "individual_noise = [normalize(noise, alpha) for noise in individual_noise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.91, 0.91, 0.92, 0.93, 0.92]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[max([model.evaluate(X_exp + noise, Y_exp) for model in models]) for noise in individual_noise]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model is .91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_res = max([model.evaluate(X_exp + noise_ensemble, Y_exp) for model in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Benchmark is .71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_folders = [\"oracle\", 'random ascent', 'greedy ascent', 'reverse hinge loss']\n",
    "colors = [\"blue\", \"green\", \"red\", \"cyan\"]\n",
    "for i, ma in enumerate(max_acc_histories):\n",
    "    plt.plot(range(100), ma, color=colors[i])\n",
    "plt.plot(range(100), [ensemble_res] * 100, linestyle=\"--\", color=\"purple\")\n",
    "# plt.legend(display_folders + [\"ensemble baseline\"], loc=\"right\", bbox_to_anchor=(1,.55), fontsize=8)\n",
    "plt.title(\"Maximum Accuracy of Binary Classifiers\")\n",
    "plt.ylabel(\"Max Accuracy\")\n",
    "plt.xlabel(\"MWU Round\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lh in loss_histories:\n",
    "    plt.plot(range(100), lh)\n",
    "plt.legend(folders, loc=\"right\", bbox_to_anchor=(1.5,.5))\n",
    "plt.title(\"Loss of Learner - Binary Experiments\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"MWU Round\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_acc_histories[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAlphaEquivalent(ensemble_model, X_exp, Y_exp, alpha, target):\n",
    "    noise =  -1 * ensemble_model.gradient(X_exp, Y_exp)\n",
    "    for i in range(noise.shape[0]):\n",
    "        noise[i] = noise[i] / np.linalg.norm(noise[i]) * alpha\n",
    "\n",
    "    res = max([model.evaluate(X_exp + noise, Y_exp) for model in models])\n",
    "    \n",
    "    if res > target:\n",
    "        return findAlphaEquivalent(ensemble_model, X_exp, Y_exp, alpha + .01, target)\n",
    "    else:\n",
    "        return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findAlphaEquivalent(ensemble_model, X_exp, Y_exp, .5, .3944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(noise.shape[0]):\n",
    "        noise[i] = noise[i] / np.linalg.norm(noise[i]) * .70\n",
    "print max([model.evaluate(X_exp + noise, Y_exp) for model in models])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
